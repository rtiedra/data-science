{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abe8522",
   "metadata": {},
   "source": [
    "NLP sentiment analysis of IMDB dataset using a LSTM recurrent neural network, licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bcf6c5",
   "metadata": {},
   "source": [
    "# Main libraries\n",
    "We start by importing the main libraires we shall use:\n",
    "\n",
    "- the [re](https://docs.python.org/3/library/re.html) module (for regular expression matching operations)\n",
    "- the [nltk](https://www.nltk.org/) toolkit (for natural language operations)\n",
    "- the [numpy](https://numpy.org/) library (for arrays operations)\n",
    "- the [pandas](https://pandas.pydata.org/) library (for data analysis)\n",
    "- the [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) module (for statistics)\n",
    "- the [matplotlib.pyplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html) interface (for MATLAB-like plots)\n",
    "\n",
    "We also download (if not already done) the \"stopwords\" and \"punkt\" data packages from the nltk toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df00e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c99aae",
   "metadata": {},
   "source": [
    "# Loading IMDB data\n",
    "We retrieve from [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download) the csv file \"IMDB Dataset.csv\" consisting of 50'000 IMDB movies and TV shows reviews with their corresponding positive or negative sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fedcca08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing the csv file into a DataFrame \"df\"\n",
    "\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada1e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mdf.columns:\u001b[0m\n",
      " Index(['review', 'sentiment'], dtype='object') \n",
      "\n",
      "\u001b[1mdf.shape:\u001b[0m\n",
      " (50000, 2) \n",
      "\n",
      "\u001b[1mdf.info():\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Printing the basic properties of \"df\" (in particular, there are no null values in \"df\")\n",
    "\n",
    "print('\\033[1m' + 'df.columns:' + '\\033[0m' + '\\n', df.columns, '\\n')\n",
    "print('\\033[1m' + 'df.shape:' + '\\033[0m' + '\\n', df.shape, '\\n')\n",
    "print('\\033[1m' + 'df.info():' + '\\033[0m')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0589d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing descriptive statistics of \"df\"\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f30dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot of the count of positive and negative reviews (they are equal)\n",
    "\n",
    "df['sentiment'].value_counts().plot.bar(color=['darkblue', 'r'], rot=0, fontsize='large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcd0848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          1\n",
       "1      A wonderful little production. <br /><br />The...          1\n",
       "2      I thought this was a wonderful way to spend ti...          1\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sentiment values to integer values (positive -> 1 and negative -> 0)\n",
    "\n",
    "df.sentiment = [1 if tag == 'positive' else 0 for tag in df.sentiment]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06a602",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "First, using regular expressions, we make the following transformations to the reviews:\n",
    "\n",
    "- remove punctuation marks\n",
    "- remove HTML tags\n",
    "- remove URL's\n",
    "- remove characters which are not letters or digits\n",
    "- remove multiple whitespaces\n",
    "- convert to lower case\n",
    "- strip whitespaces from the beginning and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ec5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing in \"before_process\" an example of review before preprocessing\n",
    "# Defining and applying the function \"process\" performing the transformations of the reviews\n",
    "# Storing in \"after_process\" the example of review after preprocessing\n",
    "\n",
    "idx = 2132\n",
    "before_process = df.iloc[idx][0]\n",
    "\n",
    "def process(x):\n",
    "    x = re.sub('[,\\.!?:()\"]', '', x)\n",
    "    x = re.sub('<.*?>', ' ', x)\n",
    "    x = re.sub('http\\S+', ' ', x)\n",
    "    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    return x.lower().strip()\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: process(x))\n",
    "after_process = df.iloc[idx][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42e9bf",
   "metadata": {},
   "source": [
    "Next, we remove stopwords from the reviews using the [word_tokenize()](https://www.nltk.org/_modules/nltk/tokenize.html#word_tokenize) function from the [nltk.tokenize]((https://www.nltk.org/api/nltk.tokenize.html) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8a008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing in \"stopwords_set\" the set of English stopwords provided by nltk\n",
    "# Defining and applying the function \"stop_w_remove\" which remove stopwords from reviews\n",
    "# Storing in \"after_removal\" the example of review after tokenization\n",
    "\n",
    "stopwords_set = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def stop_w_remove(x):\n",
    "    words = nltk.tokenize.word_tokenize(x)\n",
    "    filtered_list = [word for word in words if word not in stopwords_set]\n",
    "    return ' '.join(filtered_list)\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: stop_w_remove(x))\n",
    "after_removal = stop_w_remove(after_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f798a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExample of review before preprocessing:\u001b[0m\n",
      " I, too, found \"Oppenheimer\" to be a brilliant series and one of the finest offerings ever on American PBS. David Suchet was particularly effective as Edward Teller, as I recall, and the overall conception was spectacularly good. The only reason that the series doesn't rate a full 10/10 is for the low-budget production values in some areas. Actual content is absolutely first-rate in my recollection.<br /><br />The Oppenheimer miniseries will be released in the UK on July 31st! It will be a Region 2/PAL set, but it would seem that a Region 1/NTSC set should be soon in the offing.<br /><br />If you have a universal player in the US, you can order the series right now from Amazon UK.<br /><br />http://tinyurl.com/znyyq<br /><br />Huzzah!! \n",
      "\n",
      "\u001b[1mSame review after preprocessing:\u001b[0m\n",
      " i too found oppenheimer to be a brilliant series and one of the finest offerings ever on american pbs david suchet was particularly effective as edward teller as i recall and the overall conception was spectacularly good the only reason that the series doesn t rate a full 10 10 is for the low budget production values in some areas actual content is absolutely first rate in my recollection the oppenheimer miniseries will be released in the uk on july 31st it will be a region 2 pal set but it would seem that a region 1 ntsc set should be soon in the offing if you have a universal player in the us you can order the series right now from amazon uk huzzah \n",
      "\n",
      "\u001b[1mSame review after preprocessing and stopwords removal:\u001b[0m\n",
      " found oppenheimer brilliant series one finest offerings ever american pbs david suchet particularly effective edward teller recall overall conception spectacularly good reason series rate full 10 10 low budget production values areas actual content absolutely first rate recollection oppenheimer miniseries released uk july 31st region 2 pal set would seem region 1 ntsc set soon offing universal player us order series right amazon uk huzzah\n"
     ]
    }
   ],
   "source": [
    "# Printing an example of review before preprocessing, after preprocessing, and after stopwords removal\n",
    "\n",
    "print('\\033[1m' + 'Example of review before preprocessing:' + '\\033[0m' + '\\n', before_process, '\\n')\n",
    "print('\\033[1m' + 'Same review after preprocessing:' + '\\033[0m' + '\\n', after_process, '\\n')\n",
    "print('\\033[1m' + 'Same review after preprocessing and stopwords removal:' + '\\033[0m' + '\\n', after_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df6cd1",
   "metadata": {},
   "source": [
    "# Training and test sets\n",
    "First, we use the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from the [sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) module to split our data into random training and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb4a6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtrain_reviews.shape:\u001b[0m (45000,)\n",
      "\u001b[1mtest_reviews.shape:\u001b[0m (5000,)\n",
      "\u001b[1mtrain_sentiments.shape:\u001b[0m (45000,)\n",
      "\u001b[1mtest_sentiments.shape:\u001b[0m (5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_reviews, test_reviews, train_sentiments, test_sentiments = train_test_split(df['review'], df['sentiment'], test_size=0.1, random_state=42)\n",
    "\n",
    "print('\\033[1m' + 'train_reviews.shape:' + '\\033[0m', train_reviews.shape)\n",
    "print('\\033[1m' + 'test_reviews.shape:' + '\\033[0m', test_reviews.shape)\n",
    "print('\\033[1m' + 'train_sentiments.shape:' + '\\033[0m', train_sentiments.shape)\n",
    "print('\\033[1m' + 'test_sentiments.shape:' + '\\033[0m', test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7c2f9",
   "metadata": {},
   "source": [
    "Next, we use the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class from [keras.preprocessing.text](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/) module to create a dictionary of the 15'000 most frequent words present in the reviews (a unique integer is assigned to each word), and we print some of its [attributes](https://faroit.com/keras-docs/1.2.2/preprocessing/text/).\n",
    "\n",
    "(the index of the Tokenizer is computed the same way no matter how many most frequent words we use later, see this [post](https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "017041bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of documents the tokenizer was trained on:\u001b[0m\n",
      " 50000 \n",
      "\n",
      "\u001b[1mlength of the tokenizer index:\u001b[0m\n",
      " 125791 \n",
      "\n",
      "\u001b[1mFirst 50 entries of the tokenizer index:\u001b[0m\n",
      "('movie', 1) ('film', 2) ('one', 3) ('like', 4) ('good', 5) ('time', 6) ('even', 7) ('would', 8) ('really', 9) ('story', 10) ('see', 11) ('well', 12) ('much', 13) ('get', 14) ('bad', 15) ('people', 16) ('great', 17) ('also', 18) ('first', 19) ('made', 20) ('make', 21) ('way', 22) ('could', 23) ('movies', 24) ('characters', 25) ('think', 26) ('watch', 27) ('character', 28) ('films', 29) ('two', 30) ('many', 31) ('seen', 32) ('never', 33) ('love', 34) ('plot', 35) ('life', 36) ('acting', 37) ('show', 38) ('best', 39) ('know', 40) ('little', 41) ('ever', 42) ('man', 43) ('better', 44) ('end', 45) ('scene', 46) ('still', 47) ('say', 48) ('scenes', 49) ('something', 50)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 15000)\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "\n",
    "print('\\033[1m' + 'Number of documents the tokenizer was trained on:' + '\\033[0m' + '\\n', tokenizer.document_count, '\\n')\n",
    "print('\\033[1m' + 'length of the tokenizer index:' + '\\033[0m' + '\\n', len(tokenizer.word_index), '\\n')\n",
    "print('\\033[1m' + 'First 50 entries of the tokenizer index:' + '\\033[0m')\n",
    "print(*list(tokenizer.word_index.items())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fd232",
   "metadata": {},
   "source": [
    "We use the [texts_to_sequences()](https://github.com/keras-team/keras/blob/v2.9.0/keras/preprocessing/text.py#L325-L337) of the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class to convert the training and test reviews to lists of sequences of integers (tokens) \"train_rev_tokens\" and \"test_rev_tokens\", and we store in the numpy array \"num_tokens\" the lengths of the sequences included in \"train_rev_tokens\" and \"test_rev_tokens\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb5299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mnum_tokens:\u001b[0m\n",
      " [ 76 123  58 ...  61  89  81] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_rev_tokens = tokenizer.texts_to_sequences(train_reviews)\n",
    "test_rev_tokens = tokenizer.texts_to_sequences(test_reviews)\n",
    "num_tokens =  np.array([len(sequence) for sequence in train_rev_tokens + test_rev_tokens])\n",
    "\n",
    "print('\\033[1m' + 'num_tokens:' + '\\033[0m' + '\\n', num_tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570cf1af",
   "metadata": {},
   "source": [
    "If the lengths of the sequences were normally distributed, then a given length could be considered small or large when outside the interval\n",
    "\n",
    "$$\n",
    "\\hbox{mean value of num_tokens} \\pm \\hbox{2 standard deviations of num_tokens,}\n",
    "$$\n",
    "\n",
    "and lengths not belonging to this interval would only represent 5% of the elements of num_tokens (see the [68â€“95â€“99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) in statistics). Here, we follow this heuristics, and thus define an upper bound for the length of sequences accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6049ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mupper_bound:\u001b[0m 275\n",
      "\u001b[1mPercentage of elements of \"num_tokens\" smaller or equal than \"length_bound\":\u001b[0m 94.555 %\n"
     ]
    }
   ],
   "source": [
    "# Storing in \"upper_bound\" the upper bound for the length of sequences\n",
    "# Computing the percentage of elements of \"num_tokens\" smaller or equal than \"upper_bound\"\n",
    "\n",
    "upper_bound = int(np.mean(num_tokens) + 2 * np.std(num_tokens))\n",
    "percentage = stats.percentileofscore(num_tokens, upper_bound)\n",
    "\n",
    "print('\\033[1m' + 'upper_bound:' + '\\033[0m', upper_bound)\n",
    "print('\\033[1m' + 'Percentage of elements of \"num_tokens\" smaller or equal than \"length_bound\":' + '\\033[0m', percentage, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22457a47",
   "metadata": {},
   "source": [
    "Using the [pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function from [keras.preprocessing.sequence](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence) module, we transform \"train_rev_tokens\" and \"test_rev_tokens\" into a 2D numpy arrays of shape (number of sequences, upper_bound). Sequences of length smaller (resp. larger) than \"upper_bound\" are extended (resp. truncated) to have length = upper_bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "887bc847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mtrain_rev_pad.shape:\u001b[0m (45000, 275)\n",
      "\u001b[1mtest_rev_pad.shape:\u001b[0m (5000, 275) \n",
      "\n",
      "\u001b[1mExample of review after padding:\u001b[0m\n",
      "\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0   136\n",
      "  1216  3217  2802    27     1   123     5    44   272     1    17   471\n",
      "  9398  3217  2802  1153   324  2319   612   217     7     1  2227  1603\n",
      "  2077   293  2926    61  1712  2788    24  9889    35  4989  3024  3415\n",
      "  2997   900   285  1050  1009  3031   204   182  5818  1050   622   142\n",
      "   866   275  1390   331  2802     5   558   493   652  5294    37  3404\n",
      "   290  3428  6283   149   268   152    94   123   553   596   323   266\n",
      "    55  2210     7  1227    27  2804  2446  3835   128     2   257  2650\n",
      "   854   363  1167  1871    27    13   106  3217  2802    29   715    78\n",
      "    40  3031  3217 10627 10228  2243  5072 12696    73  3513  2095   136\n",
      "  1105   412  7023  2863   722   243   628   469  2541    62   360   814\n",
      "  1715 10551  1006   900   285   562  5071  3141    96  2077  2700  1046\n",
      "   228  9069    55   265     9  7807   485   378  2013  7807   485]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_rev_pad = pad_sequences(train_rev_tokens, maxlen=upper_bound)\n",
    "test_rev_pad = pad_sequences(test_rev_tokens, maxlen=upper_bound)\n",
    "\n",
    "print('\\033[1m' + 'train_rev_pad.shape:' + '\\033[0m', train_rev_pad.shape)\n",
    "print('\\033[1m' + 'test_rev_pad.shape:' + '\\033[0m', test_rev_pad.shape, '\\n')\n",
    "print('\\033[1m' + 'Example of review after padding:' + '\\033[0m' + '\\n\\n', train_rev_pad[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeeae40",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "We start by importing some classes from Keras:\n",
    "\n",
    "- the [Sequential](https://keras.io/api/models/sequential/) class from the keras.models API (to group a linear stack of layers into a model)\n",
    "- the [Embedding](https://keras.io/api/layers/core_layers/embedding/) class from the keras.layers API (to turn positive integers (indexes) into dense vectors of fixed size)\n",
    "- the [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) class from the keras.layers API (to apply a long short-term memory layer to an input)\n",
    "- the [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) class from the keras.layers API (to apply dropout to the input)\n",
    "- the [Dense](https://keras.io/api/layers/core_layers/dense/) class from the keras.layers API (to apply a regular densely-connected NN layer to an input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d82377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69f7cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.1\n",
    "embedding_size = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=15000, output_dim=embedding_size, input_length=upper_bound))\n",
    "model.add(LSTM(units=16, return_sequences=True))\n",
    "model.add(Dropout(r))\n",
    "model.add(LSTM(units=8, return_sequences=True))\n",
    "model.add(Dropout(r))\n",
    "model.add(LSTM(units=4))\n",
    "model.add(Dropout(r))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc73242",
   "metadata": {},
   "source": [
    "We give a summary of the model using the [summary](https://keras.io/api/models/model/#summary-method) method of the model class of Keras. The \"None\" value stands for the (not yet defined) value of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbb1c44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 275, 50)           750000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 275, 16)           4288      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 275, 16)           0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 275, 8)            800       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 275, 8)            0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 4)                 208       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 755,301\n",
      "Trainable params: 755,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
